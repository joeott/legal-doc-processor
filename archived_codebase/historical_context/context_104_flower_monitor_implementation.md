# Context 104: Flower Monitor Implementation - Complete Documentation

## Implementation Summary

This document continues from `context_104_flower_monitor.md` and documents the successful implementation of the comprehensive monitoring solution for the OCR Document Processing Pipeline.

## What Was Built

### 1. Pipeline Monitor Scripts

#### A. `pipeline_monitor.py` (Original Version)
- Full-featured monitoring dashboard as specified
- Had import path issues due to complex module dependencies
- Serves as the reference implementation

#### B. `standalone_pipeline_monitor.py` (Production Version)
- Same functionality as the original but with robust import handling
- Works reliably without complex module dependencies
- **This is the recommended version for actual use**
- Features implemented:
  - Auto-refreshing dashboard (configurable interval)
  - Comprehensive metric collection from all data sources
  - Error handling with graceful degradation
  - Clean, organized display inspired by Flower's UI

### 2. Flower Integration

#### `start_flower_monitor.sh`
```bash
#!/bin/bash
# Start Flower monitoring dashboard for Celery
```
- Automated script to launch Flower web UI
- Includes pre-flight checks:
  - Redis connectivity verification
  - Celery worker detection
  - Helpful error messages with remediation steps
- Launches Flower on http://localhost:5555

### 3. Comprehensive Documentation

#### `MONITORING_README.md` (Now moved to ai_docs)
Complete guide covering:
- All monitoring tools and their purposes
- Usage instructions with examples
- Dashboard layout documentation
- Best practices for monitoring
- Troubleshooting guidelines
- Integration with development and production workflows

## Implementation Details

### Core Architecture

The monitoring solution follows the architecture specified in the original request:

```python
class StandalonePipelineMonitor:
    def __init__(self, refresh_interval: int = 10):
        # Initialize connections to all services
        self.db_manager = SupabaseManager()
        self.redis_manager = get_redis_manager()
        self.redis_client = self.redis_manager.get_client()
        self.cache_metrics = CacheMetrics(self.redis_manager)
        
    def run(self):
        # Main monitoring loop
        while True:
            all_stats = {
                'supabase_queue': self.get_supabase_queue_stats(),
                'celery_queues': self.get_celery_queue_stats(),
                'redis_cache': self.get_redis_cache_stats(),
                'database_tables': self.get_database_table_stats(),
                'pipeline_throughput': self.get_pipeline_throughput()
            }
            self.display_dashboard(all_stats)
            time.sleep(self.refresh_interval)
```

### Metrics Implemented

All requested metrics from the original specification were implemented:

#### A. Supabase Document Processing Queue
- âœ… Total items by status (pending, processing, failed, completed)
- âœ… Items with retry_count >= max_retries
- âœ… Average age of pending items
- âœ… Items processing longer than threshold (configurable)
- âœ… Recent failed items with error messages

#### B. Celery Monitoring (Flower-inspired)
- âœ… Queue lengths for all defined queues (default, ocr, text, entity, graph)
- âœ… Active task counts from Redis
- âœ… Task categorization by type
- âœ… Redis-based queue introspection

#### C. Redis Cache & State
- âœ… Connection status (ping check)
- âœ… Cache metrics (hits, misses, hit rate)
- âœ… Document state key counts
- âœ… Processing lock counts
- âœ… Textract job status key counts

#### D. Database Table Metrics
- âœ… source_documents breakdown by status
- âœ… neo4j_documents breakdown by processingStatus
- âœ… textract_jobs breakdown by job_status
- âœ… Counts for all graph-related tables

#### E. Pipeline Health & Throughput
- âœ… Documents completed in last hour/24 hours
- âœ… Average processing time
- âœ… P95 processing time
- âœ… Overall error rate calculation

### Display Features

The dashboard implementation includes:

1. **Clear Visual Hierarchy**
   - Section headers with separators
   - Indented sub-metrics
   - Status emojis for quick visual scanning

2. **Status Emojis**
   ```python
   STATUS_EMOJIS = {
       'pending': 'â³',
       'processing': 'âš™ï¸',
       'completed': 'âœ…',
       'failed': 'âŒ',
       'healthy': 'ðŸŸ¢',
       'warning': 'ðŸŸ¡',
       'error': 'ðŸ”´'
   }
   ```

3. **Human-Readable Time Formatting**
   - Converts minutes to appropriate units (m/h/d)
   - Shows "N/A" for missing data

4. **Error Handling**
   - Graceful degradation when services unavailable
   - Error messages displayed within dashboard
   - Automatic retry on transient failures

## Usage Guide

### Basic Usage

```bash
# Start with default settings (10 second refresh)
python scripts/standalone_pipeline_monitor.py

# Custom refresh interval
python scripts/standalone_pipeline_monitor.py --refresh-interval 5

# Custom stalled threshold
python scripts/standalone_pipeline_monitor.py --stalled-threshold 15

# Combined options
python scripts/standalone_pipeline_monitor.py --refresh-interval 5 --stalled-threshold 15
```

### Dashboard Sections

1. **Header**
   - Current timestamp
   - Refresh interval
   - Monitor uptime

2. **Document Processing Queue**
   - Status distribution
   - Performance indicators
   - Stalled document alerts

3. **Celery Task Queues**
   - Queue depths
   - Active task tracking
   - Task type distribution

4. **Redis Cache & State**
   - Connection health
   - Cache performance
   - State management metrics

5. **Database Tables**
   - Document counts
   - Processing stage distribution
   - Entity and relationship statistics

6. **Pipeline Throughput**
   - Completion rates
   - Processing time metrics
   - Error rate monitoring

7. **Recent Failures**
   - Latest failed documents
   - Error message previews

## Integration Points

### With Existing Monitoring Tools

The new monitor complements existing tools:

- **live_monitor.py**: Simple status display â†’ Comprehensive dashboard
- **health_check.py**: Binary health check â†’ Detailed metrics
- **redis_monitor.py**: Redis-specific â†’ Integrated view
- **pipeline_analysis.py**: Historical analysis â†’ Real-time monitoring

### With Celery/Flower

When Celery is activated:
1. Use `standalone_pipeline_monitor.py` for overall pipeline view
2. Use Flower (http://localhost:5555) for Celery-specific details
3. Both tools complement each other

### Development Workflow

Recommended terminal setup:
```bash
# Terminal 1: Redis
docker run -d -p 6379:6379 redis:alpine

# Terminal 2: Pipeline Monitor
python scripts/standalone_pipeline_monitor.py

# Terminal 3: Queue Processor
python scripts/queue_processor.py

# Terminal 4: Celery Workers (when activated)
./scripts/start_celery_workers.sh

# Terminal 5: Flower (when using Celery)
./scripts/start_flower_monitor.sh
```

## Best Practices

### Monitoring Strategy

1. **Continuous Monitoring**
   - Run monitor during active processing
   - Use tmux/screen for persistent sessions
   - Set appropriate refresh intervals (5-10s for active monitoring)

2. **Alert Thresholds**
   - Stalled documents: > 30 minutes
   - Error rate: > 5%
   - Queue depth: > 100 items
   - Cache hit rate: < 80%

3. **Performance Optimization**
   - Increase refresh interval for remote connections
   - Monitor during different load conditions
   - Track metrics over time for trending

### Troubleshooting

Common issues and solutions:

1. **Import Errors**
   - Use `standalone_pipeline_monitor.py` instead of `pipeline_monitor.py`
   - Ensure running from project directory

2. **Connection Failures**
   - Verify Redis is running: `redis-cli ping`
   - Check Supabase credentials in `.env`
   - Test network connectivity

3. **No Data Display**
   - Ensure documents exist in database
   - Verify queue processor is running
   - Check database permissions

## Future Enhancements

Potential improvements identified during implementation:

1. **Metrics Export**
   - Prometheus endpoint for metrics
   - CSV/JSON export functionality
   - Historical data persistence

2. **Alerting**
   - Email/Slack notifications
   - Configurable alert rules
   - Alert history tracking

3. **Web UI Version**
   - Flask/FastAPI web interface
   - Real-time WebSocket updates
   - Historical charts and graphs

4. **Advanced Analytics**
   - Predictive queue depth modeling
   - Anomaly detection
   - Performance regression alerts

## Conclusion

The implementation successfully delivers all requirements from the original specification:

âœ… Comprehensive real-time monitoring dashboard
âœ… Flower-inspired design and functionality
âœ… All specified metrics implemented
âœ… Robust error handling
âœ… Easy integration with existing tools
âœ… Production-ready with documentation

The `standalone_pipeline_monitor.py` provides a reliable, feature-complete monitoring solution that gives operators full visibility into the OCR document processing pipeline's health and performance.

## Post-Implementation Fixes

After initial testing, the following issues were identified and fixed:

### 1. Supabase Column Name Corrections
- Changed `documentid` to `document_id`
- Changed `last_error` to `error_message`
- Used the schema extraction script to verify correct column names

### 2. Redis Cache Key Corrections
- Fixed cache key patterns by using `CacheKeys.get_pattern()` method instead of non-existent `_PREFIX` attributes
- Correctly used:
  - `CacheKeys.DOC_STATE` instead of `CacheKeys.DOC_STATE_PREFIX`
  - `CacheKeys.DOC_PROCESSING_LOCK` instead of `CacheKeys.DOC_PROCESSING_LOCK_PREFIX`
  - `CacheKeys.TEXTRACT_JOB_STATUS` instead of `CacheKeys.TEXTRACT_JOB_STATUS_PREFIX`

### 3. Schema Verification Tool
Created `extract_current_schema.py` to dynamically extract the current Supabase schema, which helped identify the correct column names and structure.

These fixes ensure the monitor works correctly with the actual database schema and Redis key structure.

---

# Appendix: Complete Monitoring Guide

The following sections provide the complete monitoring guide that was originally in `MONITORING_README.md`:

## Pipeline Monitoring Guide

This guide covers the monitoring tools available for the OCR Document Processing Pipeline.

### Overview

The pipeline provides multiple monitoring solutions:

1. **Standalone Pipeline Monitor** - Comprehensive real-time dashboard
2. **Flower Web UI** - Celery-specific monitoring (when using distributed processing)
3. **Live Monitor** - Simple document processing monitor
4. **Health Check** - System health verification

### 1. Standalone Pipeline Monitor

The main monitoring tool that provides a comprehensive view of the entire pipeline.

#### Features

- **Supabase Queue Monitoring**
  - Queue status counts (pending, processing, completed, failed)
  - Average pending time
  - Stalled document detection
  - Recent failure tracking

- **Celery Task Monitoring**
  - Queue lengths for each task type
  - Active task counts
  - Task distribution by type

- **Redis Cache Monitoring**
  - Connection status
  - Cache hit/miss rates
  - Document state tracking
  - Processing lock counts

- **Database Statistics**
  - Document counts by status
  - Table sizes and growth
  - Processing stage distribution

- **Performance Metrics**
  - Hourly/daily throughput
  - Average processing times
  - P95 processing times
  - Error rates

#### Dashboard Layout Example

```
================================================================================
ðŸ” OCR Document Processing Pipeline Monitor (Standalone)
ðŸ“… 2025-05-25 10:30:45 | â±ï¸  Refresh: 10s
â³ Uptime: 0:05:23
================================================================================

ðŸ“‹ Document Processing Queue (Supabase)
----------------------------------------
  â³ Pending: 12
  âš™ï¸ Processing: 3
  âœ… Completed: 245
  âŒ Failed: 2
  â° Avg Pending Age: 2.5m
  âš ï¸  Max Retries Reached: 1
  ðŸš¨ Stalled Items (>30m): 0

ðŸŽ¯ Celery Task Queues (Redis)
----------------------------------------
  ðŸ“­ default: 0
  ðŸ“¥ ocr: 5
  ðŸ“¥ text: 3
  ðŸ“­ entity: 0
  ðŸ“­ graph: 0
  ðŸ”„ Active Tasks: 8
  ðŸ“Š Active by Type:
     - ocr: 5
     - text: 3

ðŸ’¾ Redis Cache & State
----------------------------------------
  ðŸŸ¢ Connection: Connected
  ðŸ“Š Cache Performance:
     - Hits: 1234
     - Misses: 56
     - Hit Rate: 95.7%
  ðŸ“„ Document States: 15
  ðŸ”’ Processing Locks: 3
  ðŸ“‹ Textract Jobs: 2

ðŸ—„ï¸  Database Tables (Supabase)
----------------------------------------
  ðŸ“„ Source Documents: 260
     - pending: 12
     - processing: 3
     - completed: 243
     - failed: 2
  ðŸ”— Neo4j Documents: 243
  ðŸ“ Chunks: 4,521
  ðŸ‘¤ Entity Mentions: 12,345
  ðŸ¢ Canonical Entities: 3,456
  ðŸ”— Relationships: 8,901

ðŸ“ˆ Pipeline Throughput & Performance
----------------------------------------
  âœ… Completed (1h): 45
  âœ… Completed (24h): 243
  â±ï¸  Avg Processing Time: 3.2m
  â±ï¸  P95 Processing Time: 8.5m
  ðŸŸ¢ Error Rate (24h): 0.8%

âŒ Recent Failures
----------------------------------------
  â€¢ doc_abc123: Textract timeout after 5 minutes...
  â€¢ doc_def456: Entity extraction failed: OpenAI rate limit...

================================================================================
Press Ctrl+C to exit
```

### 2. Flower Web Monitoring (For Celery)

When Celery workers are active, Flower provides a web-based monitoring interface.

#### Starting Flower

```bash
# Start Flower dashboard
./scripts/start_flower_monitor.sh

# Or manually
celery -A scripts.celery_app flower --port=5555
```

#### Features

- Real-time worker status
- Task execution history
- Queue lengths and trends
- Worker performance metrics
- Task failure analysis
- REST API for programmatic access

#### Accessing Flower

Open http://localhost:5555 in your browser

#### Flower Dashboard Sections

1. **Workers** - View all active workers and their status
2. **Tasks** - Task execution history and statistics
3. **Broker** - Queue information and message counts
4. **Monitor** - Real-time graphs and metrics
5. **Events** - Live event stream from workers

### 3. Other Monitoring Tools

#### Live Monitor
```bash
python monitoring/live_monitor.py
```
Simple real-time view of document processing status.

#### Health Check
```bash
python scripts/health_check.py
```
Quick system health verification.

#### Redis Monitor
```bash
python monitoring/redis_monitor.py
```
Detailed Redis performance and state monitoring.

#### Pipeline Analysis
```bash
python monitoring/pipeline_analysis.py
```
Analyzes pipeline performance and identifies bottlenecks.

### Integration with Deployment

#### Development Setup
```bash
# Terminal 1: Start Redis
docker run -d -p 6379:6379 redis:alpine

# Terminal 2: Start Celery workers (if using distributed mode)
./scripts/start_celery_workers.sh

# Terminal 3: Start Flower (if using Celery)
./scripts/start_flower_monitor.sh

# Terminal 4: Start Pipeline Monitor
python scripts/standalone_pipeline_monitor.py

# Terminal 5: Start Queue Processor
python scripts/queue_processor.py
```

#### Production Considerations
- Use process managers (systemd, supervisor) for service management
- Configure appropriate log rotation
- Set up monitoring alerts based on thresholds
- Consider running monitors on dedicated instances

### Customization and Extension

The standalone monitor can be extended by modifying:

1. **Metrics Collection** - Add new queries in `get_*_stats()` methods
2. **Display Format** - Modify `display_dashboard()` method
3. **Alert Thresholds** - Adjust class constants
4. **Additional Integrations** - Add new data sources

Example of adding a custom metric:
```python
def get_custom_metric(self) -> Dict[str, Any]:
    """Add your custom metric collection here"""
    try:
        # Your metric collection logic
        return {'custom_value': 42}
    except Exception as e:
        return {'error': str(e)}
```

### Performance Considerations

- **Query Optimization**: Limit result sets for better performance
- **Caching**: Consider caching slow queries
- **Network Latency**: Adjust refresh intervals for remote connections
- **Resource Usage**: Monitor the monitor's own resource consumption