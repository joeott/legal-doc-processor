# Context 114: Celery Pipeline Error Testing and Conformance Report

## Executive Summary

This document details the findings from end-to-end testing of the Celery-based document processing pipeline on 2025-05-26. The testing identified and resolved several critical issues preventing successful document processing. The pipeline core functionality is now operational, with OCR and initial text processing working correctly. However, some edge cases around document reprocessing remain to be addressed.

## Testing Environment

- **Date**: 2025-05-26
- **Codebase**: phase_1_2_3_process_v5
- **Test Documents**: PDFs from `/input/` directory
- **Infrastructure**: 
  - Celery 5.5.2 with Redis broker
  - Redis Cloud (redis-12696.c325.us-east-1-4.ec2.redns.redis-cloud.com:12696)
  - AWS Textract for OCR
  - Supabase PostgreSQL database

## Issues Identified and Fixed

### 1. Import Path Errors (FIXED)
**Issue**: Module imports using relative paths failed when Celery workers loaded tasks.
```python
# Error: ModuleNotFoundError: No module named 'config'
from config import TEXTRACT_CONFIDENCE_THRESHOLD
```
**Fix**: Changed all imports to use absolute paths from project root.
```python
from scripts.config import TEXTRACT_CONFIDENCE_THRESHOLD
```
**Files Modified**:
- `scripts/supabase_utils.py` (line 768)
- All files identified in context_113 requiring import fixes

### 2. Database Constraint Violations (FIXED)
**Issue**: Invalid value for `textract_job_status` constraint.
```
Error 23514: new row violates check constraint "source_documents_textract_job_status_check"
```
**Root Cause**: Code attempted to set `textract_job_status = 'initiating'` which is not in the allowed enum values.
**Fix**: Changed to use valid status value `'submitted'` in `scripts/celery_tasks/ocr_tasks.py` (line 107).

### 3. Redis API Misuse (FIXED)
**Issue**: AttributeError when calling expire on RedisManager instead of Redis client.
```python
redis_mgr.expire(state_key, 7 * 24 * 3600)  # Error: no attribute 'expire'
```
**Fix**: Changed to call expire on the Redis client instance.
```python
redis_mgr.get_client().expire(state_key, 7 * 24 * 3600)
```
**File Modified**: `scripts/celery_tasks/ocr_tasks.py` (line 37)

### 4. Celery Queue Configuration (FIXED)
**Issue**: Tasks submitted to specific queues (ocr, text, entity, graph) but worker only listening to default queue.
**Fix**: Started worker with all queues:
```bash
celery -A scripts.celery_app worker --loglevel=info --concurrency=2 -Q default,ocr,text,entity,graph
```

### 5. Method Name Mismatch (ALREADY FIXED)
**Issue**: Test script called non-existent method `create_document_entry`.
**Status**: Already corrected to `create_source_document_entry` in current codebase.

## Current Pipeline Status

### Working Components ‚úÖ
1. **Document Submission**: Successfully creates source_documents entries
2. **Celery Task Routing**: Tasks properly routed to appropriate queues
3. **OCR Processing**: 
   - AWS Textract successfully processes PDFs
   - S3 upload and job submission working
   - Job status tracking functional
   - Text extraction completed successfully
4. **Redis State Management**: Document processing state tracked in Redis
5. **Status Updates**: Database status fields update correctly through pipeline

### Partially Working Components ‚ö†Ô∏è
1. **Text Processing Stage**:
   - Successfully creates neo4j_documents entries on first run
   - Chunking logic executes but encounters issues with:
     - Duplicate key constraints on reprocessing
     - Proto-chunk alignment warnings (may be normal for complex documents)

### Untested Components üîÑ
1. **Entity Extraction**: Not reached due to text processing blocks
2. **Entity Resolution**: Not tested
3. **Graph Building**: Not tested
4. **Neo4j Export**: Not tested

## Remaining Issues

### 1. Duplicate Key Constraints
**Problem**: Reprocessing documents fails with duplicate key violations.
```
duplicate key value violates unique constraint "neo4j_documents_documentid_key"
duplicate key value violates unique constraint "uq_document_chunk_index"
```
**Root Cause**: Pipeline lacks logic to handle existing processed data.
**Impact**: Cannot rerun failed documents without manual cleanup.

### 2. Document Reprocessing Logic
**Problem**: No clear strategy for handling partially processed documents.
**Scenarios Needing Handling**:
- Document failed at entity extraction, needs to restart from that point
- Document needs complete reprocessing due to updated models
- Testing/debugging requires multiple runs on same document

## Recommendations

### Immediate Actions (High Priority)

1. **Implement Idempotent Processing**
   - Add "upsert" logic to handle existing neo4j_documents
   - Check for existing chunks before insertion
   - Consider using `ON CONFLICT` clauses or check-before-insert pattern

2. **Add Cleanup Task**
   ```python
   @app.task
   def cleanup_document_for_reprocessing(document_uuid: str):
       """Remove all derived data for a document to allow clean reprocessing"""
       # Delete in reverse dependency order
       # 1. Delete entity mentions
       # 2. Delete chunks  
       # 3. Delete neo4j_document
       # 4. Reset source_document status
   ```

3. **Enhance Error Recovery**
   - Store detailed error context in Redis
   - Implement stage-specific retry logic
   - Add ability to resume from last successful stage

### Medium Priority Improvements

1. **Processing State Machine**
   - Formalize allowed state transitions
   - Add guards against invalid state changes
   - Implement state rollback on failures

2. **Monitoring Enhancements**
   - Add metrics for processing time per stage
   - Track retry counts and failure reasons
   - Implement alerts for stuck documents

3. **Test Data Management**
   - Create test fixtures with known good outputs
   - Implement test cleanup utilities
   - Add integration test suite with fresh database

### Long-term Architecture Considerations

1. **Transaction Boundaries**
   - Consider wrapping multi-table operations in transactions
   - Implement saga pattern for distributed transaction management

2. **Event Sourcing**
   - Track all processing events for audit trail
   - Enable replay of processing from any point

3. **Schema Evolution**
   - Plan for backward compatibility
   - Version processing pipeline changes

## Testing Checklist for Full Conformance

- [x] Environment variables configured correctly
- [x] Redis connectivity verified
- [x] Celery workers start without errors
- [x] OCR processing completes successfully
- [x] Text extraction and storage works
- [ ] Chunking completes without duplicates
- [ ] Entity extraction processes all chunks
- [ ] Entity resolution creates canonical entities
- [ ] Graph relationships built correctly
- [ ] Full document processes end-to-end
- [ ] Reprocessing handles existing data gracefully
- [ ] Error recovery works at each stage
- [ ] Monitoring shows accurate status

## Conclusion

The Celery-based pipeline infrastructure is fundamentally sound. All major components are properly connected and communicate successfully. The primary remaining work involves handling edge cases around document reprocessing and ensuring idempotent operations throughout the pipeline.

With the fixes applied during this testing session, the system can successfully process new documents through OCR and begin text processing. Completing the improvements recommended above will result in a robust, production-ready document processing pipeline.